# -*- coding: utf-8 -*-
"""GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AzzpseGbDtlGW8vrtt3zDuoIXVcoPgAK
"""



import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms
from PIL import Image
import os
import torch.optim as optim
import matplotlib.pyplot as plt
import torch.nn.functional as F
import numpy as np
from torchvision.utils import save_image


class MiniBatchDiscrimination(nn.Module):
    def __init__(self, in_features, out_features, intermediate_features=10):
        super(MiniBatchDiscrimination, self).__init__()
        self.T = nn.Parameter(torch.randn(in_features, out_features, intermediate_features))

    def forward(self, x):
        # x shape: (batch_size, in_features)
        M = x.mm(self.T.view(x.size(1), -1))  # Compute M matrix (batch_size, out_features * intermediate_features)
        M = M.view(-1, x.size(0), self.T.size(1))  # Reshape to (batch_size, batch_size, out_features)
        M_diff = M.unsqueeze(0) - M.unsqueeze(1)  # Compute pairwise differences
        M_exp = torch.exp(-torch.sum(torch.abs(M_diff), dim=2))  # Apply exponential decay

        # Sum over samples and reshape to match batch size
        M_sum = M_exp.sum(dim=1)  # Shape: (batch_size, out_features)

        # Concatenate the original input with the mini-batch discrimination output
        return torch.cat([x, M_sum], dim=1)

class Generator(nn.Module):
    def __init__(self, z_dim, img_channels, num_classes, feature_g):
        super(Generator, self).__init__()
        self.label_emb = nn.Embedding(num_classes, num_classes)

        self.gen = nn.Sequential(
            self._block(z_dim + num_classes, feature_g * 16, 4, 1, 0),  # 4x4
            self._block(feature_g * 16, feature_g * 8, 4, 2, 1),  # 8x8
            self._block(feature_g * 8, feature_g * 4, 4, 2, 1),  # 16x16
            self._block(feature_g * 4, feature_g * 2, 4, 2, 1),  # 32x32
            nn.ConvTranspose2d(feature_g * 2, img_channels, kernel_size=4, stride=2, padding=1),  # 64x64
            nn.Tanh()  # Output: 64x64
        )

    def _block(self, in_channels, out_channels, kernel_size, stride, padding):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(True)
        )

    def forward(self, noise, labels):
        labels = self.label_emb(labels).view(labels.size(0), -1, 1, 1)  # Reshape labels
        labels = labels.expand(labels.size(0), labels.size(1), noise.size(2), noise.size(3))
        x = torch.cat([noise, labels], 1)
        return self.gen(x)

class MiniBatchDiscrimination(nn.Module):
    def __init__(self, in_features, out_features, intermediate_features=10):
        super(MiniBatchDiscrimination, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.intermediate_features = intermediate_features
        self.T = nn.Parameter(torch.randn(in_features, out_features, intermediate_features))

    def forward(self, x):
        # x shape: (batch_size, in_features)
        batch_size = x.size(0)

        # Compute matrix multiplication between input and tensor
        # Reshape T to (in_features, out_features * intermediate_features)
        matrices = x.mm(self.T.view(self.in_features, -1))

        # Reshape to (batch_size, out_features, intermediate_features)
        matrices = matrices.view(batch_size, self.out_features, self.intermediate_features)

        # Compute pairwise L1 distance
        M_diff = torch.unsqueeze(matrices, 0) - torch.unsqueeze(matrices, 1)
        M_diff = torch.abs(M_diff).sum(dim=-1)

        # Apply exponential decay
        M_exp = torch.exp(-M_diff)

        # Sum over batch dimension (excluding self-interaction)
        M_sum = M_exp.sum(dim=1) - 1  # Subtract 1 to exclude self-interaction

        # Concatenate with input features
        return torch.cat([x, M_sum], dim=1)  # Now dimensions will match

class Discriminator(nn.Module):
    def __init__(self, img_channels, num_classes, feature_d):
        super(Discriminator, self).__init__()
        self.label_emb = nn.Embedding(num_classes, num_classes)

        self.disc = nn.Sequential(
            nn.Conv2d(img_channels + num_classes, feature_d, kernel_size=4, stride=2, padding=1),  # 32x32
            nn.LeakyReLU(0.2),
            self._block(feature_d, feature_d * 2, 4, 2, 1),  # 16x16
            self._block(feature_d * 2, feature_d * 4, 4, 2, 1),  # 8x8
            self._block(feature_d * 4, feature_d * 8, 4, 2, 1),  # 4x4
        )

        flattened_size = feature_d * 8 * 4 * 4
        # Ensure out_features matches the expected size
        self.mini_batch_discrimination = MiniBatchDiscrimination(
            in_features=flattened_size,
            out_features=64  # Changed from 100 to match expected dimensions
        )

        self.final_layer = nn.Sequential(
            nn.Linear(flattened_size + 64, 1),  # Adjusted input size to match concatenated features
            nn.Sigmoid()
        )

    def _block(self, in_channels, out_channels, kernel_size, stride, padding):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(0.2)
        )

    def forward(self, x, labels):
        labels = self.label_emb(labels).unsqueeze(2).unsqueeze(3)
        labels = labels.expand(labels.size(0), labels.size(1), x.size(2), x.size(3))
        x = torch.cat([x, labels], 1)

        x = self.disc(x)
        x = x.view(x.size(0), -1)  # Flatten

        x = self.mini_batch_discrimination(x)
        return self.final_layer(x)

class SpectralNorm(nn.Module):
    def __init__(self, module):
        super(SpectralNorm, self).__init__()
        self.module = nn.utils.spectral_norm(module)

    def forward(self, x):
        return self.module(x)

class Discriminator(nn.Module):
    def __init__(self, img_channels, num_classes, feature_d):
        super(Discriminator, self).__init__()
        self.label_emb = nn.Embedding(num_classes, num_classes)
        self.disc = nn.Sequential(
            SpectralNorm(nn.Conv2d(img_channels + num_classes, feature_d, kernel_size=4, stride=2, padding=1)),
            nn.LeakyReLU(0.2),
            self._block(feature_d, feature_d * 2, 4, 2, 1),
            self._block(feature_d * 2, feature_d * 4, 4, 2, 1),
            self._block(feature_d * 4, feature_d * 8, 4, 2, 1),
            nn.Conv2d(feature_d * 8, 1, kernel_size=4, stride=1, padding=0)
        )

    def _block(self, in_channels, out_channels, kernel_size, stride, padding):
        return nn.Sequential(
            SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(0.2)
        )

    def forward(self, x, labels):
        labels = self.label_emb(labels).unsqueeze(2).unsqueeze(3)
        labels = labels.expand(labels.size(0), labels.size(1), x.size(2), x.size(3))
        x = torch.cat([x, labels], 1)
        return self.disc(x)

class SpectrogramDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        self.labels = []
        self.class_to_idx = {}

        for class_idx, class_name in enumerate(os.listdir(root_dir)):
            class_dir = os.path.join(root_dir, class_name)
            self.class_to_idx[class_name] = class_idx
            for img in os.listdir(class_dir):
                self.image_paths.append(os.path.join(class_dir, img))
                self.labels.append(class_idx)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('L')  # Convert to grayscale
        label = self.labels[idx]
        if self.transform:
            image = self.transform(image)
        return image, label

# Define transformations
transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

# Create dataset and dataloaders
train_dataset = SpectrogramDataset('E:\\Msc\\Lab\\spectrum_sharing_system\\all_data\\train', transform=transform)
test_dataset = SpectrogramDataset('E:\\Msc\\Lab\\spectrum_sharing_system\\all_data\\test', transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

torch.backends.cudnn.benchmark = False

def hinge_loss_discriminator(disc_real, disc_fake):
    return torch.mean(torch.nn.ReLU()(1.0 - disc_real)) + torch.mean(torch.nn.ReLU()(1.0 + disc_fake))

def hinge_loss_generator(disc_fake):
    return -torch.mean(disc_fake)

# Training Setup
z_dim = 100
lr = 2e-4
img_channels = 1
num_classes = len(train_dataset.class_to_idx)
feature_g = 64
feature_d = 64
num_epochs = 800

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
gen = Generator(z_dim, img_channels, num_classes, feature_g).to(device)
disc = Discriminator(img_channels, num_classes, feature_d).to(device)
criterion = nn.BCELoss()
opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))
opt_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))

fixed_noise = torch.randn(num_classes, z_dim, 1, 1).to(device)
fixed_labels = torch.zeros(num_classes, dtype=torch.long).to(device)

# Training Loop
for epoch in range(num_epochs):
    for batch_idx, (real, labels) in enumerate(train_loader):
        real, labels = real.to(device), labels.to(device)
        noise = torch.randn((real.size(0), z_dim, 1, 1)).to(device)
        fake_labels = torch.randint(0, num_classes, (real.size(0),)).to(device)
        fake = gen(noise, fake_labels)

        disc_real = disc(real, labels).reshape(-1)
        disc_fake = disc(fake.detach(), fake_labels).reshape(-1)
        loss_disc = hinge_loss_discriminator(disc_real, disc_fake)
        opt_disc.zero_grad()
        loss_disc.backward()
        opt_disc.step()

        # Train Generator: hinge loss
        output = disc(fake, fake_labels).reshape(-1)
        loss_gen = hinge_loss_generator(output)
        opt_gen.zero_grad()
        loss_gen.backward()
        opt_gen.step()

        if batch_idx % 10 == 0:
            print(f'Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(train_loader)} \
                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}')

        # Generate samples at each epoch
    if epoch % 50 == 0:
      with torch.no_grad():
        fake = gen(fixed_noise, fixed_labels).detach().cpu()
        num_rows = (num_classes + 7) // 8
        fig, axs = plt.subplots(num_rows, 8, figsize=(15, 15))

      #fig, axs = plt.subplots(num_classes // 8, 8, figsize=(15, 15))
        for i in range(num_classes):
          row = i // 8
          col = i % 8
          axs[row, col].imshow(fake[i, 0, :, :], cmap='gray')  # Ensure fake[i] is correctly indexed
          axs[row, col].axis('off')
          axs[row, col].set_title(f'Class {fixed_labels[i].item()}')
        plt.tight_layout()
        plt.show()




# Save model weights within the runtime
torch.save(gen.state_dict(), 'generator_weights.pth')
torch.save(disc.state_dict(), 'discriminator_weights.pth')

# Save model weights to Google Drive
'''torch.save(gen.state_dict(), '/content/drive/MyDrive/weights/GAN_weights_250_epochs/GAN_discriminator_weights.pth')
torch.save(disc.state_dict(), '/content/drive/MyDrive/weights/GAN_weights_250_epochs/GAN_generator_weights.pth')'''


'''
"""# Generate Images"""

# Generate and Save Images
gen_weights_path = 'generator_weights.pth'

# Initialize the generator model and load the weights
gen = Generator(z_dim, img_channels, num_classes, feature_g).to(device)
gen.load_state_dict(torch.load(gen_weights_path))
gen.eval()
save_dir = '/content/drive/MyDrive/generated_images_GAN/100_epochs'
os.makedirs(save_dir, exist_ok=True)

with torch.no_grad():
    for class_label in range(num_classes):
        class_dir = os.path.join(save_dir, f'class_{class_label}')
        os.makedirs(class_dir, exist_ok=True)

        for i in range(10):
            noise = torch.randn(1, z_dim, 1, 1).to(device)
            labels = torch.tensor([class_label]).to(device)
            generated_image = gen(noise, labels).detach().cpu()


with torch.no_grad():
    for class_label in range(num_classes):
        class_dir = os.path.join(save_dir, f'class_{class_label}')
        os.makedirs(class_dir, exist_ok=True)

        for i in range(100):
            noise = torch.randn(1, z_dim, 1, 1).to(device)
            labels = torch.tensor([class_label]).to(device)
            generated_image = gen(noise, labels).detach().cpu()

            # Save the generated image
            img_path = os.path.join(class_dir, f'image_{i}.png')
            torchvision.utils.save_image(generated_image, img_path)



class_names = [
    'ch1_collision_ch2_empty', 'ch1_collision_ch2_secondary', 'ch1_empty_ch2_collision', 'ch1_empty_ch2_empty',
    'ch1_empty_ch2_primary', 'ch1_empty_ch2_secondary', 'ch1_primary_ch2_collision', 'ch1_primary_ch2_empty',
    'ch1_primary_ch2_primary', 'ch1_primary_ch2_secondary', 'ch1_secondary_ch2_empty', 'ch1_secondary_ch2_primary'
]

z_dim = 100  # latent dimension for the GAN
img_channels = 1  # number of channels in generated image
num_classes = len(class_names)
feature_g = 64  # feature size for generator
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



save_dir = 'E:\\Msc\\Lab\\spectrum_sharing_system\\fid_data\GAN'
os.makedirs(save_dir, exist_ok=True)

# Generate and save images
with torch.no_grad():
    print(class_names)
    for class_label, class_name in enumerate(class_names):
        class_dir = os.path.join(save_dir, class_name)
        os.makedirs(class_dir, exist_ok=True)

        for i in range(100):
            noise = torch.randn(1, z_dim, 1, 1).to(device)
            labels = torch.tensor([class_label]).to(device)
            generated_image = gen(noise, labels).detach().cpu()

            # Save the generated image
            img_path = os.path.join(class_dir, f'image_{i}.png')
            torchvision.utils.save_image(generated_image, img_path)

print("Images generated and saved successfully.")
'''
